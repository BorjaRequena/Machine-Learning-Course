{
 "cells": [
  {
   "cell_type": "raw",
   "id": "0be4143a",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Logistic regression\"\n",
    "author: \"Alexandre Dauphin\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "jupyter: python3\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87cc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Use this cell only in google colab \n",
    "#! pip install git+https://github.com/BorjaRequena/Neural-Network-Course.git\n",
    "#! pip install nbdev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c5c41",
   "metadata": {},
   "source": [
    "<a href=\"https://githubtocolab.com/BorjaRequena/Neural-Network-Course/blob/master/lectures_nb/02_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "from ipywidgets import interact\n",
    "\n",
    "from lectures_ml.losses import BCE\n",
    "from lectures_ml.optimizers import sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce74629",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e75388",
   "metadata": {},
   "source": [
    "In this section, we consider the classification task. In particular, we focus here on the binary classification, whcih means that a datapoint can either be in class $0$ or in class $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328e414",
   "metadata": {},
   "source": [
    "Let us consider a concrete dataset, the [toxity dataset](https://online.stat.psu.edu/stat462/node/208/). In this dataset, scientists were studying the toxity of a substance to a population of insects. They therefore administrated a serie of toxic doses and measured the number of deaths in the population of $250$ individuals. The results of the study are shown in @fig-bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ind = 250\n",
    "data = dict(doses=range(1,7),deaths=[28,53,93,126,172,197])\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df['p_dying'] = df['deaths']/n_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8342158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-bar\n",
    "#| fig-cap: Number of deaths with respect to the number of doses.\n",
    "#| code-fold: true\n",
    "px.bar(data_frame=df, x='doses', y='deaths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20890f",
   "metadata": {},
   "source": [
    "Here, the goal of the classification would be to predict, given a number of dosis whether an individual would be death (class $0$) or alive (class $1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5c5447",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81275b1",
   "metadata": {},
   "source": [
    "We can here define a probability distribution of dying for each value of the doses by dividing by the total bnumber of individuals (here $250$). We can therefore see the classification task as fitting this probability distribution. A simple trial function is the sigmoid function\n",
    "\n",
    "$$ \\sigma(x) =\\frac{1}{1+e^{-(ax+b)}}$$\n",
    "and the goal is to find the values of $a$ and $b$ that fit the best the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bdbba5",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "\n",
    "Notice how we transformed our classification problem into a regression problem. We will come back to this probabilisitc view of machine lerning in the [next lecture](probabilistic_view.ipynb).            \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02875591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x,a,b):\n",
    "    return 1/(1 + np.exp(-(a*x+b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25427069",
   "metadata": {},
   "source": [
    "@figfig-fitsigmoid shows a trial of the fit for the values of $a=0.5$ and $b=-2$. You can play around with the values of $a$ and $b$ to find a more decent fit in the next figure (only in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea43308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-fitsigmoid\n",
    "#| fig-cap: Probability of deaths with respect to the number of doses.\n",
    "#| code-fold: true\n",
    "a, b = 0.5, -2\n",
    "\n",
    "vecd = np.arange(0,7,0.1)\n",
    "\n",
    "Fig = go.Figure()\n",
    "\n",
    "\n",
    "Fig.add_bar(x=df['doses'], y=df['p_dying'], name='toxity dataset')\n",
    "\n",
    "Fig.add_trace(go.Scatter(name=f'sigmoid: a={a}, b={b}',x=vecd,y=sigmoid(vecd,a,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "def hlines(a,b):\n",
    "    vecd = np.arange(0,7,0.1)\n",
    "    Fig = go.FigureWidget()\n",
    "    Fig.add_bar(x=df['doses'], y=df['p_dying'],name=\"dataset\")\n",
    "    Fig.add_trace(\n",
    "        go.Scatter(\n",
    "            line=dict(color=\"#00CED1\", width=4),\n",
    "            name=\"Sigmoid\",\n",
    "            x=vecd,\n",
    "            y=sigmoid(vecd,a,b))\n",
    "        )\n",
    "    \n",
    "    Fig.update_yaxes(range=[0, 1])\n",
    "\n",
    "    Fig.show()\n",
    "\n",
    "interact(hlines, a=(-5,5,0.1), b=(-5,5,0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcae7fd",
   "metadata": {},
   "source": [
    "# The dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbc6c3",
   "metadata": {},
   "source": [
    "Until now, we have considered the viewpoint of the whole population. But generally, the dataset is built from the data of each individual to which is associated a tuple $(x,y)$, where $x$ is the number of dosis and $y$ is the class ($0$: dead, $1$: alive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f768d7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in zip(df['doses'], df['deaths']):\n",
    "    vec1 = np.zeros((n_ind,2))\n",
    "    vec1[:,0], vec1[:d,1] = i, 1\n",
    "    vec = vec1 if i ==1 else np.concatenate((vec,vec1))\n",
    "    \n",
    "np.random.shuffle(vec)\n",
    "x, y = vec[:,0], vec[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1bd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:20])\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1aaa45",
   "metadata": {},
   "source": [
    "#  Loss function: Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7b878b",
   "metadata": {},
   "source": [
    "We now want to automatically find the values of $a$ and $b$. To this end, we have to define a loss function. We will here use the binary cross entropy\n",
    "\n",
    "$$ BCE = -\\frac{1}{N}\\sum_{i=1}^N y_i \\log [\\sigma(x_i,a,b)] +(1-y_i)\\log[1-\\sigma(x_i,a,b)].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd6e81",
   "metadata": {},
   "source": [
    "Let us have a look of the loss landscape of the binary cross entropy with respect to the paramters $a$ and $b$, as depicted in @fig-loss_landscape. We actually observe that the landscape is convex for this choice of the Loss function. In this particular case, the latter can proven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1014c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "#| code-summary: Code to generate the data for the plot\n",
    "\n",
    "vec_a = np.arange(-5,5,0.1)\n",
    "vec_b = np.arange(-5,5,0.1)\n",
    "matz = np.zeros((vec_a.size,vec_b.size))\n",
    "\n",
    "for i, a1 in enumerate(vec_a):\n",
    "    for j, b1 in enumerate(vec_b):\n",
    "        p = dict(a=a1, b=b1)\n",
    "        matz[i,j] = BCE(x, y, sigmoid, params=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823b679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-loss_landscape\n",
    "#| fig-cap: Binary Cross Entropy\n",
    "#| code-fold: true\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_contour(z=matz,x=vec_b, y=vec_a,hovertemplate=\n",
    "                    'a:%{y:.2f}'\n",
    "                    +'<br>b:%{x:.2f}</br>'\n",
    "                    +'f:%{z:.2f}<extra></extra>')\n",
    "\n",
    "\n",
    "d = dict(width=600,\n",
    "         height=600,\n",
    "         xaxis={'title':'b'},\n",
    "         yaxis={'title':'a'}\n",
    "       )\n",
    "\n",
    "fig.update_layout(d)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e246869",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "\n",
    "## Exercise \n",
    "\n",
    "What is the value loss for your guess? Is it close to the minimum?\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55362bdb",
   "metadata": {},
   "source": [
    "#  Gradient of the Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4f72f8",
   "metadata": {},
   "source": [
    "Let us compute the gradient with respect to $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d247",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "\n",
    "## Exercise\n",
    "\n",
    "What is the gradient of the sigmoid function $\\sigma(x)$? \n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b6ebd",
   "metadata": {},
   "source": [
    "Although the problem is convex, there is no easy way to derive an analytical closed form. This comes from the non-linearity introduced by the sigmoid. Libraries like `scikit-learn`propose to use different algorithm such as e.g. conjugate gradient. In the next section, we will consider the stochastic gradient descent approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef9febf",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2df0b0",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Exercise\n",
    "\n",
    "Implement the function `grad_BCE(x, y, p)` where `p=dict(a=a, b=b)` is a dictionary containing the the parameters $a$ and $b$. The function should return an array of the form `np.array([grad_a, grad_b])`. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166c273",
   "metadata": {},
   "source": [
    "Having computed the gradients, we can now apply the stochastic gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79af5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a0, b0 = 2, 1 \n",
    "\n",
    "pini = dict(a=a0, b=b0)\n",
    "ll = dict(loss=BCE, grads=grad_BCE, fun=sigmoid)\n",
    "\n",
    "trackers = sgd(x, y, pini, ll, niter=int(1E2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb1fa3b",
   "metadata": {},
   "source": [
    "@fig-sgd shows the trajectory of the stochastic gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41bd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: fig-sgd\n",
    "#| fig-cap: Trajectory in the parameter space of the stochastic gradient descent algorithm.\n",
    "#| code-fold: true\n",
    "\n",
    "amin, amax = np.min(trackers['a']), np.max(trackers['a'])\n",
    "bmin, bmax = np.min(trackers['b']), np.max(trackers['b'])\n",
    "\n",
    "n = 100\n",
    "stepa, stepb =(amax-amin)/n, (bmax-bmin)/n\n",
    "\n",
    "vec_a = np.arange(amin-19*stepa, amax+20*stepa, stepa)\n",
    "vec_b = np.arange(bmin-19*stepb, bmax+20*stepb,stepb)\n",
    "matz = np.zeros((vec_a.size,vec_b.size))\n",
    "\n",
    "for i, a1 in enumerate(vec_a):\n",
    "    for j, b1 in enumerate(vec_b):\n",
    "        p = dict(a=a1, b=b1)\n",
    "        matz[i,j] = BCE(x, y, sigmoid, params=p)\n",
    "        \n",
    "fig = go.Figure()\n",
    "fig.add_contour(z=matz,x=vec_b, y=vec_a,\n",
    "                hovertemplate=\n",
    "                    'a:%{y:.2f}'\n",
    "                    +'<br>b:%{x:.2f}</br>'\n",
    "                    +'f:%{z:.2f}<extra></extra>')\n",
    "mask = np.arange(0,len(trackers['a']),100)\n",
    "veca, vecb, vecl = np.array(trackers['a'])[mask],np.array(trackers['b'])[mask], np.array(trackers['loss'])[mask]\n",
    "fig.add_scatter(x=vecb, y=veca, name=f'SGD',text=vecl, mode='lines+markers',\n",
    "                hovertemplate=\n",
    "                'a:%{y:.2f}'\n",
    "                +'<br>b:%{x:.2f}</br>'\n",
    "                +'f:%{text:.2f}<extra></extra>')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1288a00f",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2885f6",
   "metadata": {},
   "source": [
    "So far, we did not discuss about the choice of the metric. The first metric that comes into mind is the accuracy, i.e.\n",
    "\n",
    "$$ \\text{acc}= \\frac{\\# \\text{correct predictions}}{\\#\\text{dataset}},$$\n",
    "\n",
    "which considers the ratio between the number of correct predictions and the number of elements in our training set.\n",
    "\n",
    "let us compute the accuracuy in terms of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b0741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "veca, vecb, vecl = np.array(trackers['a']), np.array(trackers['b']), np.array(trackers['loss'])\n",
    "mask = np.arange(0,len(veca),100)\n",
    "veca, vecb, vecl = veca [mask], vecb[mask], vecl[mask] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dbce4d",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Exercise\n",
    "\n",
    "Write the function `accuracy(x,y,a,b)`, which returns the accuracy for a given dataset and for the parameters $a$ and $b$. Choose the label with the following rule: $0$ if $\\sigma_i<0.5$ else $1$.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d36115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: true\n",
    "def accuracy(x,y,a,b):\n",
    "    yp = sigmoid(x, a, b)\n",
    "    yp[yp>0.5] = 1\n",
    "    yp[yp<=0.5] = 0\n",
    "    return np.sum(yp==y)/y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed75cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_acc = np.zeros_like(veca)\n",
    "\n",
    "for i,(a,b) in enumerate(zip(veca,vecb)):\n",
    "    vec_acc[i] = accuracy(x,y,a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7adf280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=vecl, name=\"Loss\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=vec_acc, name=\"Accuracy\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"iterations\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Loss\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "yp = sigmoid(x, a, b)\n",
    "yp[yp>0.5] = 1\n",
    "yp[yp<=0.5] = 0\n",
    "cm = confusion_matrix(y,yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y =[\"Dead\", \"Alive\"], [\"Dead\", \"Alive\"]\n",
    "fig = px.imshow(cm, x=X, y=Y, text_auto=True,color_continuous_scale='Blues')\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65385ed",
   "metadata": {},
   "source": [
    "# 2D dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f16f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 2, 1\n",
    "nsamples = 200\n",
    "x = np.random.rand(200,2)*4-2\n",
    "y = x[:,1] < a*x[:,0]+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(x[:,0].min(),x[:,0].max(),0.1)\n",
    "plt.scatter(x[:,0],x[:,1],c=y[:])\n",
    "plt.plot(v,a*v+b)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c0a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0,1\n",
    "x[:,1] = x[:,1] + np.random.normal(loc=mu, scale=sigma, size=nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b039d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(x[:,0].min(),x[:,0].max(),0.1)\n",
    "plt.scatter(x[:,0],x[:,1],c=y[:])\n",
    "plt.plot(v,a*v+b)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36105650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

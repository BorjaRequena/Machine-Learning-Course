"""This module contains the loss functions and the gradients"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/01_losses.ipynb.

# %% auto 0
__all__ = ['MSE', 'grad_MSE_lr', 'grad_MSE_pr', 'BCE', 'grad_BCE', 'L_per', 'grad_per']

# %% ../nbs/lib_nbs/01_losses.ipynb 2
import numpy as np
from .data_gen import curve

# %% ../nbs/lib_nbs/01_losses.ipynb 4
def MSE(x:np.ndarray, # x data of N elements
        y:np.ndarray, # y data of N elements
        fun:callable, # function $y=f(x)$
        params=None, # Parameters of the function in the form of a dictionary
       )->float:
    '''Given the data $x$ and $y$, this function computes the mean square error between $y$ and $y'=f(x)$'''
    yp = fun(x) if params is None else fun(x,**params)
    MSE = np.mean((y-yp)**2)
    return MSE

# %% ../nbs/lib_nbs/01_losses.ipynb 6
def grad_MSE_lr(x:np.ndarray, # x data of N elements
                y:np.ndarray, # y data of N elements
                params: dict, # Parameters of the function
            )-> np.ndarray: # gradients
    '''Computes the gradient of the mean square error loss function with respect to $a$ and $b$ and returns np.array([$\partial_a$ MSE,$\partial_b$ MSE])'''
    a, b = params['a'], params['b']
    yp = a*x+b
    ga, gb = np.mean(2*x*(yp-y)), np.mean(2*(yp-y))
    return np.array([ga,gb])

# %% ../nbs/lib_nbs/01_losses.ipynb 7
def grad_MSE_pr(x:np.ndarray, # x data of N elements
                y:np.ndarray, # y data of N elements
                params: dict, # parameters of the function
            )-> np.ndarray: # gradients
    '''Computes the gradient of the mean square error loss function with respect to $a$ and $b$ and returns np.array([$\partial_a$ MSE,$\partial_b$ MSE])'''
    coeffs = params['coeffs']
    yp = curve(x, **params)
    ll = -2*(y-yp)
    g = []
    for i in range(len(coeffs)):
        g.append(np.mean(ll*x**i))
    return [np.array(g)]

# %% ../nbs/lib_nbs/01_losses.ipynb 8
def BCE(x:np.ndarray, # x data of N elements
        y:np.ndarray, # y data of N elements
        fun:callable, # function $y=f(x)$
        params: dict # Parameters of the function
       )->float:
    '''Given the data $x$ and $y$, this function computes the mean binary cross entropy $y$ and $y'=f(x)$'''
    yp = fun(x) if params is None else fun(x,**params)
    l = y*np.log(yp) + (1-y)*np.log(1-yp)
    return -np.mean(l)

# %% ../nbs/lib_nbs/01_losses.ipynb 9
def grad_BCE(x:np.ndarray, # x data of N elements
             y:np.ndarray, # y data of N elements
             params: dict, # Parameters of the function
            )-> np.ndarray: # gradients
    '''Computes the gradient of the binary cross entropy loss function with respect to $a$ and $b$ and returns np.array([$\partial_a$ BCE,$\partial_b$ BCE])'''
    def sigmoid(x,a,b):
        return 1/(1 + np.exp(-(a*x+b)))
    a, b = params['a'], params['b']
    yp = sigmoid(x, a, b)
    ga, gb = np.mean((yp-y)*x), np.mean((yp-y))
    return np.array([ga,gb])

# %% ../nbs/lib_nbs/01_losses.ipynb 10
def L_per(x:np.ndarray, # x data of N elements
       y:np.ndarray, # y data of N elements
       fun:callable, # function $y=f(x)$
       params: dict # Parameters of the function
      )->float:
    '''Given the data $x$ and $y$, this function computes the Loss of the perceptron algorithm'''
    l = -fun(x,**params)*y
    ll = np.heaviside(l,0)
    ll = np.mean(ll*l)
    return ll

# %% ../nbs/lib_nbs/01_losses.ipynb 11
def grad_per(x:np.ndarray, # x data of N elements
             y:np.ndarray, # y data of N elements
             params: dict, # Parameters of the function
            )-> np.ndarray: # gradients
    '''Computes the gradient of the perceptron loss function and returns `np.array(grad_w)`'''
    w = params['w']
    l = -(w[0]+x@w[1:])*y
    ll = np.heaviside(l,0)
    xx,yy = np.copy(x), np.copy(y)
    yy= yy*ll
    xx[:,0], xx[:,1] = xx[:,0]*yy, xx[:,1]*yy 
    gradw0 = -np.mean(yy)
    gradw = -np.mean(xx,axis=0)
    return [np.concatenate([[gradw0],gradw])]

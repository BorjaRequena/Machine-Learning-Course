{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85618949",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multiclass classification\"\n",
    "author: \"Marcin Płodzień\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "jupyter: python3\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d07f19",
   "metadata": {},
   "source": [
    "# One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54174181",
   "metadata": {},
   "source": [
    "Let's consider problem of data classification when each trainig sample $\\vec{x}$ has a label $y$ belonging to one class, where we have $J$ classes in total. Next, we can enumerate each class by index $j \\in \\{1,\\dots, J\\}$.\n",
    "\n",
    "Multiclass classification problems can be considered as task for which each input sample $\\vec{x}$ is equiped with the discrete probability distribution \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p = [p_1, p_2, \\dots, p_J],\\\\\n",
    "\\sum_{i=j}^{J} p_j = 1,\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "providing information what is the probability that given imput data $\\vec{x}$ belongs to given class $j$.\n",
    "\n",
    "In a particular scenario of a labeled data sample  $\\vec{x}$ with label $y$ belonging to class with number $j = 3$ the corresponding probability distribution is\n",
    "\n",
    "\\begin{equation}\n",
    " p = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n",
    "\\end{equation}\n",
    "\n",
    "As such, training sample $\\vec{x}$ is equiped with a new label $p$.\n",
    "\n",
    "Such a maping between labels $y$ to discrete probability distribution $p$ in the literature is so-called ${\\it one-hot}$ encoding.\n",
    "\n",
    "One-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6df8ae7",
   "metadata": {},
   "source": [
    "# Categorical cross-entropy as a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87411995",
   "metadata": {},
   "source": [
    "For multiclass classification task architecture of the Neural Network has output layer with $J$ nodes, corresponding to number of all classes in the training dataset. We interpret output of a neural network $\\phi = [\\phi_1, \\phi_2, \\dots, \\phi_J]$ as a predicted discrete probability distribution $q = [q_1, q_2, \\dots, q_J]$, after applying softmax activation function\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_j \\to q_j = softmax(\\phi_j) = \\frac{e^{\\phi_j}}{\\sum_l e^{\\phi_j}},\n",
    "\\end{equation}\n",
    "\n",
    "which assures that $\\sum_j q_j = 1$. As such, output $q = \\{q_0, q_2, \\dots, q_J\\}$ can be interpreted as a discrete probability distribution, as well as $p$.\n",
    "\n",
    "Now, the class prediction is taken as index $l$ corresponding to the maximum value of the class probability, i.e. the model prediction is a index $l$ for which $q_l$ has maximal value - i.e. it provides digit which is most likely a proper label for the input data.\n",
    "\n",
    "During training the Neural Network we want to minimize distance between input class probability distribution $p$ and predicted class probability distribution $q$. \n",
    "\n",
    "To compare two probability distributaion, i.e. to have a measure how $p$ and $p$ differ, we will use earlier introduced Kullback-Leibler divergence:\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(p || q) = \\sum_{l} p_l\\log\\frac{p_l}{q_l}.\n",
    "\\end{equation}\n",
    "\n",
    "We can see that\n",
    "\n",
    "\\begin{equation}\n",
    "D_{KL}(p || q) = \\sum_{l}p_l\\log{p_l} -\\sum_l p_l\\log{q_l} \\equiv {\\cal S} - \\text{CE}(p,q),\n",
    "\\end{equation}\n",
    "\n",
    "where ${\\cal S}$ is Shannon entropy for discrete probability distribution $p$, and\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{CE}(p,q) = -\\sum_l p_l\\log{q_l}\n",
    "\\end{equation}\n",
    "is called categorical-cross entropy.\n",
    "\n",
    "Because the Shannon entropy does not depends on the trainable parameters  we can consider only categorical cross-entropy as a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e566d",
   "metadata": {},
   "source": [
    "# Example: Multiclass classification in MNIST dataset\n",
    "\n",
    "In the context of the MNIST dataset our labels are digits assigned to $28\\times28$ pixels images. In such a case, we can consider handwritten digits recognition as a classification problem with $J = 10$ different classes - each for each digit. We assume that each class has assigned arbitrary index $l$ enumerating classes.\n",
    "\n",
    "In multiclass classification problem, one of the most popular techniqe for for data labels is so called ${\\it one-hot encoding}$. One-hot encoding is a way of representing each label as a  $J$-dimensional vectors. Each vector has all elements set to $0$ except one element, whose position corresponds to arbitrary class index $l$.\n",
    "\n",
    "For example, the digit $3$ has class index $4$ (we count from $0$), thus its label would be represented as \n",
    "\n",
    "\\begin{equation}\n",
    " p = y_{\\text{one-hot-encoded}} = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] \\\\\n",
    "\\end{equation} \n",
    "\n",
    "One-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models.\n",
    "\n",
    "Let' import training data: images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef89b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "Nx = 28   # number of pixels in x-direction\n",
    "Ny = 28   # number of pixels in y-direction\n",
    "N_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b586b",
   "metadata": {},
   "source": [
    "Now, we define simple feed-forward neural network with one hidden layers with $N_{h_1} = 21$ nodes (as in our previous example) with ReLU activation function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "N_h_1 = 21\n",
    "model = nn.Sequential(nn.Linear(Nx*Ny, N_h_1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(N_h_1, N_class)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25073249",
   "metadata": {},
   "source": [
    "Now, we define loss function $L$ as cross-entropy, and Adam as a optimizer for calculating gradient of the loss function $L$ with respect to trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cfb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f03fa",
   "metadata": {},
   "source": [
    "Finally, we will train our model for $N_{\\text{epoch}} = 10$ epochs, and collect value of the loss function at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.18809032182036434\n",
      "Epoch 1 - Training loss: 0.18602984545748436\n",
      "Epoch 2 - Training loss: 0.18213596730344078\n",
      "Epoch 3 - Training loss: 0.17800354234763047\n",
      "Epoch 4 - Training loss: 0.1751984041442336\n",
      "Epoch 5 - Training loss: 0.1730396168262783\n",
      "Epoch 6 - Training loss: 0.17177876067965397\n",
      "Epoch 7 - Training loss: 0.16887598526852726\n",
      "Epoch 8 - Training loss: 0.16749640748198671\n",
      "Epoch 9 - Training loss: 0.16485213802686569\n",
      "Epoch 10 - Training loss: 0.16440106320109513\n",
      "Epoch 11 - Training loss: 0.16213823579498002\n",
      "Epoch 12 - Training loss: 0.15957567634593164\n",
      "Epoch 13 - Training loss: 0.15976456029296937\n",
      "Epoch 14 - Training loss: 0.15756025875229507\n",
      "Epoch 15 - Training loss: 0.1583266692863567\n",
      "Epoch 16 - Training loss: 0.15594374585046825\n",
      "Epoch 17 - Training loss: 0.1546545083712\n",
      "Epoch 18 - Training loss: 0.15279026868056134\n",
      "Epoch 19 - Training loss: 0.1515156587383259\n",
      "Epoch 20 - Training loss: 0.15149353574842278\n",
      "Epoch 21 - Training loss: 0.14962262904513746\n",
      "Epoch 22 - Training loss: 0.15081732090153516\n",
      "Epoch 23 - Training loss: 0.14860467597254431\n",
      "Epoch 24 - Training loss: 0.14812911951592736\n",
      "Epoch 25 - Training loss: 0.14703498993402542\n",
      "Epoch 26 - Training loss: 0.14545652714234267\n",
      "Epoch 27 - Training loss: 0.14655765508815868\n",
      "Epoch 28 - Training loss: 0.14348608946828828\n",
      "Epoch 29 - Training loss: 0.14380284551598593\n"
     ]
    }
   ],
   "source": [
    "N_epoch = 30\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(0, N_epoch):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784-dimensional vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        # One-hot encode the labels\n",
    "        one_hot_labels = torch.zeros(labels.size(0), 10)\n",
    "        one_hot_labels[torch.arange(labels.size(0)), labels] = 1\n",
    "                \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, one_hot_labels)        \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()  # automatic calculating the loss with respects to trainable parameters\n",
    "        \n",
    "        # Update the weights according to chosen optimization function. Here: Adam\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} - Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efcadf3",
   "metadata": {},
   "source": [
    "Now, we can evaluate model on a test dataset and check the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4684506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy = 94.76%')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "confusion_matrix = np.zeros((10,10))\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        # Flatten MNIST images into a 784-dimensional vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # Apply the softmax function to the output\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        \n",
    "        # Get the class with the highest probability\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        # Update the correct and total counters\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        for i in range(0, predicted.shape[0]):           \n",
    "            confusion_matrix[predicted[i].item(),labels[i].item()] += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct / total\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "correct = np.sum(np.diagonal(confusion_matrix))\n",
    "accuracy = correct/np.sum(confusion_matrix)\n",
    "confusion_matrix = confusion_matrix/np.sum(confusion_matrix)*100\n",
    "plt.imshow(confusion_matrix)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"Accuracy = \" + \"{:2.2f}\".format(accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f640b9",
   "metadata": {},
   "source": [
    "As we can see, we reach $\\sim 95 \\%$ accuracy on test data, which is a huge improvement comparing to our previous neural network with only one output node (accuracy $\\sim 59 \\%$), where we were using MSE as a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2d479a",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "### Exercise\n",
    "Add two, or three additinal layers to our network. How does it improve accuracy on a test data?\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85618949",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Multiclass classification with Neural Networks\"\n",
    "author: \"Marcin Płodzień\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "jupyter: python3\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e98769",
   "metadata": {},
   "source": [
    "# Multiclass classification task: one-hot encoding\n",
    "\n",
    "Let's consider problem of data classification when each trainig sample $\\vec{x}$ has a label $y$ belonging to one class, where we have $J$ classes in total. Next, we can enumerate each class by index $j \\in \\{1,\\dots, J\\}$.\n",
    "\n",
    "Multiclass classification problems can be considered as task for which each input sample $\\vec{x}$ is equiped with the discrete probability distribution \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p = [p_1, p_2, \\dots, p_J],\\\\\n",
    "\\sum_{i=j}^{J} p_j = 1,\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "providing the information about what is the probability that given imput data $\\vec{x}$ belongs to given class $j$.\n",
    "\n",
    "In a particular scenario of a labeled data sample  $\\vec{x}$ with label $y$ belonging to class with number $j = 3$ the corresponding probability distribution is\n",
    "\n",
    "\\begin{equation}\n",
    " p = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n",
    "\\end{equation}\n",
    "\n",
    "As such, training sample $\\vec{x}$ is equiped with a new label $p$.\n",
    "\n",
    "Such a maping between labels $y$ to discrete probability distribution $p$ in the literature is so-called ${\\it one-hot encoding}$. \n",
    "\n",
    "One-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54174181",
   "metadata": {},
   "source": [
    "# Multiclass classification task\n",
    "\n",
    "Let's consider problem of data classification when each trainig sample $\\vec{x}$ has a label $y$ belonging to one class, where we have $J$ classes in total. Next, we can enumerate each class by index $j \\in \\{1,\\dots, J\\}$.\n",
    "\n",
    "Multiclass classification problems can be considered as task for which each input sample $\\vec{x}$ is equiped with the discrete probability distribution \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "p = [p_1, p_2, \\dots, p_J],\\\\\n",
    "\\sum_{i=j}^{J} p_j = 1\n",
    "\\end{split}\n",
    "\\end{equation},\n",
    "providing information what is the probability that given imput data $\\vec{x}$ belongs to given class $j$.\n",
    "\n",
    "In a particular scenario of a labeled data sample  $\\vec{x}$ with label $y$ belonging to class with number $j = 3$ the corresponding probability distribution is\n",
    "\n",
    "\\begin{equation}\n",
    " p = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0].\n",
    "\\end{equation}\n",
    "\n",
    "As such, training sample $\\vec{x}$ is equiped with a new label $p$.\n",
    "\n",
    "Such a maping between labels $y$ to discrete probability distribution $p$ in the literature is so-called ${\\it one-hot-encoding}$. \n",
    "\n",
    "One-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87411995",
   "metadata": {},
   "source": [
    "# Categorical cross-entropy as a loss function\n",
    "\n",
    "For multiclass classification task architecture of the Neural Network has output layer with $J$ nodes, corresponding to number of all classes in the training dataset. We interpret output of a neural network $\\phi = [\\phi_1, \\phi_2, \\dots, \\phi_J]$ as a predicted discrete probability distribution $q = [q_1, q_2, \\dots, q_J]$, after applying softmax activation function\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi_j \\to q_j = softmax(\\phi_j) = \\frac{e^{\\phi_j}}{\\sum_l e^{\\phi_j}},\n",
    "\\end{equation}\n",
    "\n",
    "which assures that $\\sum_j q_j = 1$. As such, output $q = \\{q_0, q_2, \\dots, q_J\\}$ can be interpreted as a discrete probability distribution, as well as $p$.\n",
    "\n",
    "Now, the class prediction is taken as index $l$ corresponding to the maximum value of the class probability, i.e. the model prediction is a index $l$ for which $q_l$ has maximal value - i.e. it provides digit which is most likely a proper label for the input data.\n",
    "\n",
    "During training the Neural Network we want to minimize distance between input class probability distribution $p$ and predicted class probability distribution $q$. \n",
    "\n",
    "The measure for comparing probability distribution is given by the a Kullback-Leibler divergence providing measure of how two probability distributions $p$ and $q$ differ:\n",
    "\n",
    "\\begin{equation}\n",
    "K_{LB}(p || q) = \\sum_{l} p_l\\log\\frac{p_l}{q_l}.\n",
    "\\end{equation}\n",
    "\n",
    "We can see that\n",
    "\n",
    "\\begin{equation}\n",
    "K_{LB}(p || q) = \\sum_{l}p_l\\log{p_l} -\\sum_l p_l\\log{q_l} \\equiv {\\cal H} - \\text{CE}(p,q),\n",
    "\\end{equation}\n",
    "\n",
    "where ${\\cal H}$ is Shannon entropy for discrete probability distribution $p$, and\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{CE}(p,q) = -\\sum_l p_l\\log{q_l}\n",
    "\\end{equation}\n",
    "is called categorical-cross entropy.\n",
    "\n",
    "Because the Shannon entropy does not depends on the trainable parameters  we can consider only categorical cross-entropy as a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521e566d",
   "metadata": {},
   "source": [
    "# Example: Multiclass classification in MNIST dataset\n",
    "\n",
    "In the context of the MNIST dataset our labels are digits assigned to $28\\times28$ pixels images. In such a case, we can consider handwritten digits recognition as a classification problem with $J = 10$ different classes - each for each digit. We assume that each class has assigned arbitrary index $l$ enumerating classes.\n",
    "\n",
    "In multiclass classification problem, one of the most popular techniqe for for data labels is so called ${\\it one-hot encoding}$. One-hot encoding is a way of representing each label as a  $J$-dimensional vectors. Each vector has all elements set to $0$ except one element, whose position corresponds to arbitrary class index $l$.\n",
    "\n",
    "For example, the digit $3$ has class index $4$ (we count from $0$), thus its label would be represented as \n",
    "\n",
    "\\begin{equation}\n",
    " p = y_{\\text{one-hot-encoded}} = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] \\\\\n",
    "\\end{equation} \n",
    "\n",
    "One-hot encoding is often used as a way to represent categorical variables in machine learning models. It has the advantage of being able to represent any number of categories, and the labels are mutually exclusive, which can be useful for certain types of models.\n",
    "\n",
    "Let' import training data: images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef89b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "Nx = 28   # number of pixels in x-direction\n",
    "Ny = 28   # number of pixels in y-direction\n",
    "N_class = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b586b",
   "metadata": {},
   "source": [
    "Now, we define simple feed-forward neural network with three hidden layers. First hidden-layer has $N_{h_1} = 128$ nodes, second hidden-layer has $N_{h_2} = 64$ nodes, while third hidden-layer has $N_{h_3} = 32$ nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "N_h_1 = 128\n",
    "N_h_2 = 64\n",
    "N_h_3 = 32\n",
    "model = nn.Sequential(nn.Linear(Nx*Ny, N_h_1),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(N_h_1, N_h_2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(N_h_2, N_h_3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(N_h_3, N_class)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25073249",
   "metadata": {},
   "source": [
    "Now, we define loss function $L$ as cross-entropy, and Adam as a optimizer for calculating gradient of the loss function $L$ with respect to trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cfb85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7f03fa",
   "metadata": {},
   "source": [
    "Finally, we will train our model for $N_{\\text{epoch}} = 10$ epochs, and collect value of the loss function at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.44632343203623664\n",
      "Epoch 1 - Training loss: 0.19825542715392006\n",
      "Epoch 2 - Training loss: 0.14819248952368683\n",
      "Epoch 3 - Training loss: 0.12276379527675031\n",
      "Epoch 4 - Training loss: 0.10394735406708917\n",
      "Epoch 5 - Training loss: 0.08960785535397504\n",
      "Epoch 6 - Training loss: 0.08135678043622754\n",
      "Epoch 7 - Training loss: 0.07523494778917844\n",
      "Epoch 8 - Training loss: 0.06587286778553915\n",
      "Epoch 9 - Training loss: 0.05905826543847512\n"
     ]
    }
   ],
   "source": [
    "N_epoch = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(0, N_epoch):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784-dimensional vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        # One-hot encode the labels\n",
    "        one_hot_labels = torch.zeros(labels.size(0), 10)\n",
    "        one_hot_labels[torch.arange(labels.size(0)), labels] = 1\n",
    "                \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, one_hot_labels)        \n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()  # automatic calculating the loss with respects to trainable parameters\n",
    "        \n",
    "        # Update the weights according to chosem optimization function. Here: Adam\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch} - Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efcadf3",
   "metadata": {},
   "source": [
    "Now, we can evaluate model on test dataset and check the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4684506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy = 97.13%')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "confusion_matrix = np.zeros((10,10))\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        # Flatten MNIST images into a 784-dimensional vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(images)\n",
    "        \n",
    "        # Apply the softmax function to the output\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        \n",
    "        # Get the class with the highest probability\n",
    "        _, predicted = torch.max(probs, 1)\n",
    "        \n",
    "        # Update the correct and total counters\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        for i in range(0, predicted.shape[0]):           \n",
    "            confusion_matrix[predicted[i].item(),labels[i].item()] += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct / total\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(confusion_matrix)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"Accuracy = \" + \"{:2.2f}\".format(accuracy*100) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

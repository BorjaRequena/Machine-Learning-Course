{
 "cells": [
  {
   "cell_type": "raw",
   "id": "aa918f44-97f5-4b06-94c7-9c6668e82b29",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Neural Networks regularization\"\n",
    "author: \"Marcin Płodzień\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "jupyter: python3\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d552eb70",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff15868",
   "metadata": {},
   "source": [
    "Overfitting is a common problem in deep learning, and occurs when a model is excessively complex and is able to fit the training data too closely. This can lead to poor generalization to new, unseen data, as the model may have learned patterns in the training data that are not present in the real world.\n",
    "\n",
    "There are several factors that can contribute to overfitting in deep neural networks:\n",
    "\n",
    "a) Large number of parameters: complex network architectures, such as very deep networks or networks with a large number of parameters, can  be prone to overfitting.\n",
    "\n",
    "b) Lack of sufficient data: If the training dataset is small, the model may be able to fit the training data too closely, leading to overfitting.\n",
    "\n",
    "\n",
    "It is therefore to use regularization techniques to prevent it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61c9e1",
   "metadata": {},
   "source": [
    "# Neural Networks regularization\n",
    "\n",
    "Regularization techniques are used to prevent overfitting in deep learning models. Regularization techniques result in:\n",
    "\n",
    "   1. Improved generalization: By preventing overfitting, regularization can help to improve the generalization performance of the model on new, unseen data.\n",
    "\n",
    "   2. Simplified models: Regularization can help to reduce the complexity of the model, which can make it easier to interpret and understand.\n",
    "\n",
    "   3. Increased robustness: Regularized models are often more robust to noise and other types of perturbations, as they are less sensitive to specific patterns in the training data.\n",
    "\n",
    "   4. Improved efficiency: Regularization can help to reduce the number of parameters in the model, which can make the model more efficient to train and deploy.\n",
    "\n",
    "   5. Better interpretability: Regularization can help to identify the most important features in the data, which can improve the interpretability of the model.\n",
    "\n",
    "   6. Improved optimization: Regularization can help to stabilize the optimization process, which can lead to faster convergence and better performance.\n",
    "\n",
    "   7. Reduced risk of overfitting: Regularization helps to reduce the risk of overfitting, which can be a major issue when training deep learning models.\n",
    "\n",
    "   8. Better generalization to new data distributions: Regularization can help to improve the generalization of the model to new data distributions, as it encourages the model to learn more generalizable patterns in the data.\n",
    "\n",
    "   9. Improved performance on small datasets: Regularization can be particularly useful when working with small datasets, as it can help to reduce the risk of overfitting and improve the generalization performance of the model.\n",
    "\n",
    "   10. Improved performance on noisy datasets: Regularization can help to improve the performance of the model on noisy datasets, as it encourages the model to learn more robust and generalizable patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc05e4",
   "metadata": {},
   "source": [
    "# Regularization techniques\n",
    "\n",
    "There are several regularization techniques that are commonly used to train deep neural networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1748376",
   "metadata": {},
   "source": [
    "## Weight decay (L1/L2 regularization)\n",
    "\n",
    "   This method involves adding a penalty to the cost function during training to discourage the model from learning excessively large weights. These regularization techniques are based on the idea that large weights can lead to overfitting, as they may allow the model to fit the training data too closely. L1 and L2 regularization are methods for adding a penalty term to the cost function during training to discourage the model from learning excessively large weights. \n",
    "L1 regularization: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3342b8",
   "metadata": {},
   "source": [
    "### L1 regularization\n",
    "\n",
    "L1 regularization, also known as ${\\it Lasso} regularization, adds a penalty term to the cost function that is proportional to the absolute value of the weights. The L1 regularization term has the form:\n",
    "\n",
    "\\begin{equation}\n",
    "L_1 = \\lambda  \\sum |W|\n",
    "\\end{equation}\n",
    "where λ is the regularization parameter, and w is the weight.\n",
    "\n",
    "The effect of L1 regularization is to push the weights towards zero, which can help to reduce overfitting by decreasing the complexity of the model. L1 regularization can also be useful for feature selection, as it tends to drive the weights of unimportant features to zero, effectively removing them from the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dba4ff",
   "metadata": {},
   "source": [
    "### L2 regularization\n",
    "\n",
    "L2 regularization, also known as ${\\it Ridge}$ regularization, adds a penalty term to the cost function that is proportional to the square of the weights. The L2 regularization term has the form:\n",
    "\n",
    "\\begin{equation}\n",
    "L_2 = \\lambda  \\sum W^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is the regularization parameter, and $W$ are weights of the model.\n",
    "\n",
    "The effect of L2 regularization is to shrink the weights towards zero, which can help to reduce overfitting by decreasing the complexity of the model. \n",
    "\n",
    "However, unlike L1 regularization, L2 regularization does not lead to the complete removal of weights, as it only shrinks the weights rather than setting them to zero.\n",
    "\n",
    "In general, L2 regularization is more commonly used than L1 regularization, as it tends to be more stable and easier to optimize. However, L1 regularization can be useful in situations where it is important to select a subset of features, as it has the ability to drive some weights to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd950",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    " This is a regularization technique that randomly sets a fraction of the activations to zero during training. This helps to prevent overfitting by forcing the model to be more robust to the specific weights of individual units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425884f5",
   "metadata": {},
   "source": [
    "\n",
    "## Batch normalization:\n",
    "\n",
    "Batch normalization is a technique that is used to normalize the activations of a mini-batch in order to stabilize and accelerate the training of deep neural networks.\n",
    "\n",
    "\n",
    "Ideally, the normalization would be conducted over the entire training set, but to use this step jointly with stochastic optimization methods, it is impractical to use the global information. Thus, normalization is restrained to each mini-batch in the training process. This can help to prevent overfitting by ensuring that the activations of a layer have a consistent distribution, which makes it easier for the model to learn.\n",
    "\n",
    "During training, the batch normalization layer computes the mean and standard deviation of the activations of the current mini-batch and uses these statistics to normalize the activations. The normalized activations are then computed as follows:\n",
    "\n",
    "normalized_activations = (activations - mean) / std\n",
    "\n",
    "where mean and std are the mean and standard deviation of the activations, respectively.\n",
    "\n",
    "\n",
    "The batch normalization layer also stores the mean and standard deviation of the activations in a set of running statistics, which are updated at each training iteration by exponentially moving the mean and standard deviation of the mini-batch towards the mean and standard deviation of the running statistics.\n",
    "\n",
    "During evaluation of the model, the batch normalization layer uses the mean and standard deviation of the running statistics to normalize the activations. This helps to ensure that the model's behavior is consistent during training and evaluation, and can improve the model's generalization ability.\n",
    "\n",
    "\n",
    "\n",
    "In PyTorch, batch normalization can be implemented by using the BatchNorm1d layer for fully-connected layers, or the BatchNorm2d layer for convolutional layers. These layers should be placed after the linear or convolutional layers, respectively, and before the non-linear activation function. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b1af8",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "This method involves stopping the training process before the model has fully converged. This can be used to prevent overfitting by limiting the number of iterations that the model can use to learn the training data.\n",
    "\n",
    "Data augmentation: This method involves generating additional training examples by applying random transformations to the existing training examples. This can help to prevent overfitting by providing the model with more diverse data to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34ef492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# Train the model for a maximum of 100 epochs\n",
    "#for epoch in range(100):\n",
    "  # Train the model for one epoch\n",
    "#  train(model, train_data, optimizer)\n",
    "\n",
    "  # Evaluate the model on the validation set\n",
    "#  val_loss = evaluate(model, val_data)\n",
    "\n",
    "  # If the validation loss has not improved in the last 10 epochs, stop training\n",
    "#  if val_loss > best_val_loss:\n",
    "#    best_val_loss = val_loss\n",
    "#    patience = 0\n",
    "#  else:\n",
    "#    patience += 1\n",
    "#    if patience == 10:\n",
    "#      break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40656b",
   "metadata": {},
   "source": [
    "## Data augumentation\n",
    "Data augmentation: To implement data augmentation in PyTorch, you can use the torchvision.transforms module, which provides a number of pre-defined image transformations that can be applied to the training data. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "\n",
    "# Define a transformation that randomly crops and flips the input images\n",
    "#transform = transforms.Compose([\n",
    "#    transforms.RandomCrop(32, padding=4),\n",
    "#    transforms.RandomHorizontalFlip()\n",
    "#])\n",
    "\n",
    "# Apply the transformation to the training data\n",
    "#train_data = torch.utils.data.DataLoader(\n",
    "#    dataset, batch_size=batch_size, shuffle=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317765fb",
   "metadata": {},
   "source": [
    "# Example: MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cba864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983c75",
   "metadata": {},
   "source": [
    "We'll then define the transform for the data. We'll use the ToTensor transform to convert the images to tensors, and we'll also apply data augmentation by randomly flipping and rotating the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7ac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                             #   transforms.RandomRotation(7),\n",
    "                             #   transforms.RandomHorizontalFlip(),\n",
    "                             #   transforms.RandomVerticalFlip(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ecb64",
   "metadata": {},
   "source": [
    "Next, we'll load the MNIST dataset using the transform we just defined. We'll also define the batch size and set the shuffle parameter to True so that the data is shuffled at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='.', train=False, download=True, transform=transform)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb94da",
   "metadata": {},
   "source": [
    "Now, we'll define the example of our neural network architecture\n",
    "(later we can add dropout with a probability of $0.5$ after each hidden layer, and/or batch normalization after each hidden layer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "Nx = 28\n",
    "Ny = 28\n",
    "N_h_1 = 128\n",
    "N_h_2 = 64\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(Nx*Ny, N_h_1)\n",
    "        self.bn1 = nn.BatchNorm1d(N_h_1)\n",
    "        self.fc2 = nn.Linear(N_h_1, N_h_2)\n",
    "        self.bn2 = nn.BatchNorm1d(N_h_2)\n",
    "        self.fc3 = nn.Linear(N_h_2, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, Nx*Ny)\n",
    "        x = F.relu(self.fc1(x))\n",
    "     #   x = self.bn1(x)\n",
    "     #   x = F.dropout(x, p=0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "     #   x = self.bn2(x)\n",
    "     #   x = F.dropout(x, p=0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea6d2c7",
   "metadata": {},
   "source": [
    "With torchinfo library we can inspect properties of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77875ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=====================================================================================================================================================================\n",
       "Net                                      [1, 1, 28, 28]            [1, 10]                   384                       --                        --\n",
       "├─Linear: 1-1                            [1, 784]                  [1, 128]                  100,480                   --                        100,480\n",
       "├─Linear: 1-2                            [1, 128]                  [1, 64]                   8,256                     --                        8,256\n",
       "├─Linear: 1-3                            [1, 64]                   [1, 10]                   650                       --                        650\n",
       "=====================================================================================================================================================================\n",
       "Total params: 109,770\n",
       "Trainable params: 109,770\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.11\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.44\n",
       "Estimated Total Size (MB): 0.44\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# python -m pip install torchinfo\n",
    "import torchinfo\n",
    "torchinfo.summary(model, (1, 28, 28), batch_dim = 0, \n",
    "                  col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"), \n",
    "                  verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a795a94e",
   "metadata": {},
   "source": [
    "Similar results can be obtained with model.state_dict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c49bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([128, 784])\n",
      "fc1.bias \t torch.Size([128])\n",
      "bn1.weight \t torch.Size([128])\n",
      "bn1.bias \t torch.Size([128])\n",
      "bn1.running_mean \t torch.Size([128])\n",
      "bn1.running_var \t torch.Size([128])\n",
      "bn1.num_batches_tracked \t torch.Size([])\n",
      "fc2.weight \t torch.Size([64, 128])\n",
      "fc2.bias \t torch.Size([64])\n",
      "bn2.weight \t torch.Size([64])\n",
      "bn2.bias \t torch.Size([64])\n",
      "bn2.running_mean \t torch.Size([64])\n",
      "bn2.running_var \t torch.Size([64])\n",
      "bn2.num_batches_tracked \t torch.Size([])\n",
      "fc3.weight \t torch.Size([10, 64])\n",
      "fc3.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0cf70",
   "metadata": {},
   "source": [
    "Let's define custom loss function: cross-entropy with L2 regularization term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082aaec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function\n",
    "class CrossEntropyLossWithL1(nn.Module):\n",
    "    def __init__(self, weight_decay=0.01):\n",
    "        super(CrossEntropyLossWithL1, self).__init__()\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        # Compute the cross entropy loss\n",
    "        loss = self.cross_entropy_loss(logits, labels)\n",
    "        \n",
    "        # Add the L1 regularization term\n",
    "        L1_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            L1_loss += torch.sum(param.abs())\n",
    "\n",
    "        # Add the L2 regularization term\n",
    "        L2_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            L2_loss += torch.sum(param.pow(2))            \n",
    "        \n",
    "       # loss += self.weight_decay * L1_loss  \n",
    "        loss += self.weight_decay * L2_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ae288",
   "metadata": {},
   "source": [
    "Let's set custom loss function and the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()  # standard cross-entropy loss function \n",
    "criterion = CrossEntropyLossWithL1() # our custom loss function with L1 regularization term\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa027c7b",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "### Reseting gradients during training loop with  optimizer.zero_grad()\n",
    "\n",
    "In PyTorch, optimizers are used to update the model parameters based on the computed gradients. The optimizer.zero_grad() function clears the gradients of all the model parameters, so that they are ready to be updated with new gradients in the next iteration.\n",
    "\n",
    "If you don't clear the gradients before starting a new iteration, the gradients from the previous iteration will be accumulated and the model parameters will be updated based on the accumulated gradients, which can lead to incorrect results.\n",
    "\n",
    "For example, consider the case where the gradients from the previous iteration are all positive. If the gradients from the current iteration are also all positive, then the accumulated gradients will be even larger, which can lead to large updates to the model parameters that may be too large and result in overfitting.\n",
    "\n",
    "On the other hand, if the gradients from the previous iteration are all negative and the gradients from the current iteration are all positive, then the accumulated gradients will cancel each other out and the model parameters will not be updated at all, which can result in underfitting.\n",
    "\n",
    "By clearing the gradients before starting a new iteration, you ensure that the model parameters are updated based only on the gradients from the current iteration, which helps to prevent overfitting and underfitting.\n",
    "\n",
    "It is generally a good practice to call optimizer.zero_grad() before starting each iteration of the training loop.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d23a50",
   "metadata": {},
   "source": [
    "The training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09877c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 0.3990  Accuracy: 0.8820\n",
      "Epoch: 2  Loss: 0.1906  Accuracy: 0.9425\n",
      "Epoch: 3  Loss: 0.1375  Accuracy: 0.9587\n",
      "Epoch: 4  Loss: 0.1093  Accuracy: 0.9667\n",
      "Epoch: 5  Loss: 0.0939  Accuracy: 0.9712\n",
      "Epoch: 6  Loss: 0.0811  Accuracy: 0.9742\n",
      "Epoch: 7  Loss: 0.0698  Accuracy: 0.9779\n",
      "Epoch: 8  Loss: 0.0660  Accuracy: 0.9791\n",
      "Epoch: 9  Loss: 0.0608  Accuracy: 0.9804\n",
      "Epoch: 10  Loss: 0.0527  Accuracy: 0.9822\n",
      "Epoch: 11  Loss: 0.0463  Accuracy: 0.9855\n",
      "Epoch: 12  Loss: 0.0465  Accuracy: 0.9841\n",
      "Epoch: 13  Loss: 0.0428  Accuracy: 0.9856\n",
      "Epoch: 14  Loss: 0.0370  Accuracy: 0.9876\n",
      "Epoch: 15  Loss: 0.0369  Accuracy: 0.9876\n",
      "Epoch: 16  Loss: 0.0351  Accuracy: 0.9879\n",
      "Epoch: 17  Loss: 0.0322  Accuracy: 0.9889\n",
      "Epoch: 18  Loss: 0.0272  Accuracy: 0.9906\n",
      "Epoch: 19  Loss: 0.0300  Accuracy: 0.9895\n",
      "Epoch: 20  Loss: 0.0272  Accuracy: 0.9903\n",
      "Epoch: 21  Loss: 0.0303  Accuracy: 0.9899\n",
      "Epoch: 22  Loss: 0.0244  Accuracy: 0.9917\n",
      "Epoch: 23  Loss: 0.0226  Accuracy: 0.9924\n",
      "Epoch: 24  Loss: 0.0224  Accuracy: 0.9927\n",
      "Epoch: 25  Loss: 0.0243  Accuracy: 0.9918\n",
      "Epoch: 26  Loss: 0.0216  Accuracy: 0.9925\n",
      "Epoch: 27  Loss: 0.0210  Accuracy: 0.9927\n",
      "Epoch: 28  Loss: 0.0213  Accuracy: 0.9929\n",
      "Epoch: 29  Loss: 0.0231  Accuracy: 0.9922\n",
      "Epoch: 30  Loss: 0.0179  Accuracy: 0.9939\n"
     ]
    }
   ],
   "source": [
    "# Set the number of epochs\n",
    "n_epochs = 30\n",
    "\n",
    "# Lists to store the running loss and accuracy\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    # Loop over the training data\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the running loss and correct predictions\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        running_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate the epoch loss and accuracy\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = running_correct / len(train_loader.dataset)\n",
    "\n",
    "    # Print the epoch loss and accuracy\n",
    "    print('Epoch: {}  Loss: {:.4f}  Accuracy: {:.4f}'.format(epoch+1, epoch_loss, epoch_acc))\n",
    "\n",
    "    # Append the epoch loss and accuracy to the lists\n",
    "    loss_list.append(epoch_loss)\n",
    "    acc_list.append(epoch_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846c801",
   "metadata": {},
   "source": [
    "To evaluate the model on the test dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39e63b",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "### Model evaluation: model.eval() and torch.no_grad()\n",
    "\n",
    "In PyTorch, the model.eval() function sets the model to evaluation mode. In evaluation mode, the model's batch normalization and dropout layers behave differently than they do in training mode.\n",
    "\n",
    "For example, in training mode, the batch normalization layer uses the mean and standard deviation of the layer's input to normalize the activations, whereas in evaluation mode it uses the mean and standard deviation of the running statistics stored during training to normalize the model's parameters. This helps to ensure that the model's behavior is consistent during training and evaluation.\n",
    "\n",
    "In addition, dropout layers behave differently in training mode and evaluation mode. In training mode, dropout layers randomly drop out a certain fraction of the activations to prevent overfitting. In evaluation mode, dropout layers do not drop out any activations and simply pass the activations through unchanged.\n",
    "\n",
    "The torch.no_grad() function is used to turn off gradient calculation. In general, gradient calculation is needed only during training, when you want to update the model parameters based on the computed gradients. During evaluation, you don't need to compute the gradients and turning off gradient calculation can save some computation time.\n",
    "\n",
    "By using model.eval() and torch.no_grad() together, you can ensure that the model is in evaluation mode and that gradient calculation is turned off during evaluation. \n",
    "\n",
    "It is generally a good practice to set the model to evaluation mode before evaluating the model's performance, and to set the model back to training mode before continuing with the training.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc4bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1029 Test Accuracy: 0.9773\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the test loss and correct predictions\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "\n",
    "# Turn off gradient calculation\n",
    "confusion_matrix = np.zeros((10,10))\n",
    "with torch.no_grad():\n",
    "    # Loop over the test data\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Update the test loss and correct predictions\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "        for label_index, label in enumerate(labels):\n",
    "            confusion_matrix[predicted[label_index].item(),label.item()] += 1\n",
    "\n",
    "# Calculate the test loss and accuracy\n",
    "test_loss = test_loss / len(test_loader)\n",
    "test_acc = test_correct / len(test_loader.dataset)\n",
    "\n",
    "# Print the test loss and accuracy\n",
    "print('Test Loss: {:.4f} Test Accuracy: {:.4f}'.format(test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d86c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy = 97.73%')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correct = np.sum(np.diagonal(confusion_matrix))\n",
    "accuracy = correct/np.sum(confusion_matrix)\n",
    "confusion_matrix = confusion_matrix/np.sum(confusion_matrix)*100\n",
    "plt.imshow(confusion_matrix)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"Accuracy = \" + \"{:2.2f}\".format(accuracy*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ff7d22",
   "metadata": {},
   "source": [
    "Finally, we can have a look at trained weights and biasases via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc9e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#    if param.requires_grad:\n",
    "#        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bf4d1",
   "metadata": {},
   "source": [
    "Finally, we can save our trained model to file with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"model_save\"\n",
    "torch.save(model, PATH);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00acb8a1",
   "metadata": {},
   "source": [
    "We can also load model from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(PATH);\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb3c4a",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "### Exercise\n",
    "1. Calculate confusion matrix for training data.\n",
    "2. Plot accuracy/loss on training data vs epochs (the same for test data).\n",
    "3. Add dropout layers - check accuracy on test data.\n",
    "4. Add batch-norm layers - check accuracy on test data.\n",
    "5. Train on augumented data (rotations/flips).\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

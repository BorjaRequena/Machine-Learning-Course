{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Neural networks with PyTorch\"\n",
    "author: \"Borja Requena\"\n",
    "toc: true\n",
    "number-sections: true\n",
    "highlight-style: pygments\n",
    "jupyter: python3\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://githubtocolab.com/BorjaRequena/Neural-Network-Course/blob/master/nbs/course/deep_learning/pytorch_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch basics\n",
    "\n",
    "[PyTorch](https://pytorch.org) is an automatic differentiation framework that, essentially, is your [NumPy](https://numpy.org) for machine learning and anything that involves exact derivatives.\n",
    "PyTorch natively supports hardware accelerators, such as GPUs, that can significantly speed up matrix multiplication operations, as well as distributed computing to handle large workloads.\n",
    "\n",
    "The main element of PyTorch is a tensor, which behaves very similarly to NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "b_tensor = torch.tensor([4.0, 5.0, 6.0])\n",
    "type(a_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform any kind of operations over tensors, from matrix to element-wise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor @ b_tensor  # dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have `requires_grad` a property that indicates whether gradients should be computed with respect to their values.\n",
    "By default, this is set to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we set it to `True`, we will be able to compute the gradient of scalar quantities with respect to the tensor.\n",
    "Let's consider a simple example where we add the sine and cosine of both tensors:\n",
    "$$\\sum_i\\sin(a_i) + \\cos(b_i)$$\n",
    "\n",
    "::: {.callout-note}\n",
    "# Derivatives\n",
    "Recall that $\\frac{d}{dx}\\sin(x) = \\cos(x)$ and $\\frac{d}{dx}\\cos(x) = -\\sin(x)$.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor.requires_grad = True\n",
    "b_tensor.requires_grad = True\n",
    "\n",
    "value = torch.sum(torch.sin(a_tensor) + torch.cos(b_tensor))\n",
    "value.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the sum, `value`, is also a tensor.\n",
    "When we call the `backward` method, it computes the gradient over all the tensors that have been involved in its calculation.\n",
    "The resulting gradients are stored in the tensors themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5403, -0.4161, -0.9900]),\n",
       " tensor([ 0.5403, -0.4161, -0.9900], grad_fn=<CosBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tensor.grad, torch.cos(a_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7568, 0.9589, 0.2794]),\n",
       " tensor([0.7568, 0.9589, 0.2794], grad_fn=<NegBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tensor.grad, -torch.sin(b_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-warning}\n",
    "## Zero out gradients\n",
    "\n",
    "Subsequent gradient computations with respect to the same tensor will add the new gradient to the previous one.\n",
    "We must take this into account and reset the gradients manually when needed. \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the gradient of another quantity with respect to the same tensors will modify its gradient.\n",
    "Consider the sum of all the entries of $\\mathbf{a}$.\n",
    "The gradient with respect to itself is 1 for every entry.\n",
    "This value will be added to the previously existing gradient, although $\\mathbf{b}$ will not be affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.5403, 0.5839, 0.0100]),\n",
       " tensor([1.5403, 0.5839, 0.0100], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_sum = torch.sum(a_tensor)\n",
    "a_sum.backward()\n",
    "\n",
    "a_tensor.grad, torch.cos(a_tensor) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7568, 0.9589, 0.2794])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_tensor.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reset the gradients of a tensor, we can manually set them to `None` or zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tensor.grad.zero_()\n",
    "b_tensor.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "In machine learning applications, we hardly ever zero out gradients at the tensor level.\n",
    "We typically rely on the `zero_grad()` method from either our [optimizer](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer) or [module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) to reset the gradients.\n",
    "See [the docs](https://pytorch.org/tutorials/recipes/recipes/zeroing_out_gradients.html) for further details.\n",
    "::: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
